{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use conda-amazonei-tensorflow-p36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import sagemaker\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.tuner import ContinuousParameter, HyperparameterTuner\n",
    "from sagemaker.tensorflow.serving import Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import img_to_array, load_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert images to tfRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5840\n"
     ]
    }
   ],
   "source": [
    "data_root = pathlib.Path(os.path.join(os.getcwd(), 'chest_xray_standard'))\n",
    "all_image_paths = list(data_root.glob('t*/*/*'))\n",
    "all_image_paths = [str(path) for path in all_image_paths]\n",
    "random.shuffle(all_image_paths)\n",
    "image_count = len(all_image_paths)\n",
    "\n",
    "all_image_paths[:10]\n",
    "print(len(all_image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NORMAL', 'PNEUMONIA', 'PNEUMONIA', 'PNEUMONIA', 'PNEUMONIA', 'PNEUMONIA', 'PNEUMONIA', 'NORMAL', 'PNEUMONIA', 'NORMAL']\n",
      "[0, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "all_image_labels = [pathlib.Path(path).parent.name\n",
    "                    for path in all_image_paths]\n",
    "print(all_image_labels[:10])\n",
    "all_image_labels = list(map((lambda v: 1 if v == 'PNEUMONIA' else 0), \n",
    "                            all_image_labels))\n",
    "print(all_image_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numpy array with assoc labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 3\n",
    "image_height = 224\n",
    "image_width = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 images to array\n",
      "500 images to array\n",
      "750 images to array\n",
      "1000 images to array\n",
      "1250 images to array\n",
      "1500 images to array\n",
      "1750 images to array\n",
      "2000 images to array\n",
      "2250 images to array\n",
      "2500 images to array\n",
      "2750 images to array\n",
      "3000 images to array\n",
      "3250 images to array\n",
      "3500 images to array\n",
      "3750 images to array\n",
      "4000 images to array\n",
      "4250 images to array\n",
      "4500 images to array\n",
      "4750 images to array\n",
      "5000 images to array\n",
      "5250 images to array\n",
      "5500 images to array\n",
      "5750 images to array\n",
      "All images to array!\n"
     ]
    }
   ],
   "source": [
    "dataset = np.ndarray(shape=(image_count, image_height, image_width, channels),\n",
    "                     dtype=np.uint8)\n",
    "\n",
    "i = 0\n",
    "for file in all_image_paths:\n",
    "    img = load_img(file)  # this is a PIL image\n",
    "    img = img.resize((image_width, image_height))\n",
    "    x = img_to_array(img, 'channels_last') \n",
    "    dataset[i] = x\n",
    "    i += 1\n",
    "    if i % 250 == 0:\n",
    "        print(\"%d images to array\" % i)\n",
    "print(\"All images to array!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset, \n",
    "                                                    all_image_labels, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tfrecord(images, labels, num_examples, name, directory):\n",
    "    def _int64_feature(value):\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "    def _bytes_feature(value):\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    if images.shape[0] != num_examples:\n",
    "        raise ValueError('Images size %d does not match label size %d.' % (images.shape[0], num_examples))\n",
    "    rows = images.shape[1]\n",
    "    cols = images.shape[2]\n",
    "    depth = images.shape[3]\n",
    "\n",
    "    filename = os.path.join(directory, name + '.tfrecords')\n",
    "    print('Writing', filename)\n",
    "    writer = tf.python_io.TFRecordWriter(filename)\n",
    "    for index in range(num_examples):\n",
    "        image_raw = images[index].tobytes()\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'height': _int64_feature(rows),\n",
    "            'width': _int64_feature(cols),\n",
    "            'depth': _int64_feature(depth),\n",
    "            'label': _int64_feature(labels[index]),\n",
    "            'image_raw': _bytes_feature(image_raw)}))\n",
    "        writer.write(example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_tfrecord(X_train, y_train, len(y_train), \n",
    "                    'chest_xray_images_train', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_tfrecord(X_test, y_test, len(y_test), \n",
    "                    'chest_xray_images_test', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload the train and test .tfrecords files to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del all_image_paths\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "job_name_prefix = 'pneumonia-detection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = sagemaker_session.upload_data(path='/home/ec2-user/SageMaker/chest_xray_images_train.tfrecords', \n",
    "                                           key_prefix='{0}/input/tfrecord/train'.format(job_name_prefix))\n",
    "test_path = sagemaker_session.upload_data(path='/home/ec2-user/SageMaker/chest_xray_images_test.tfrecords', \n",
    "                                          key_prefix='{0}/input/tfrecord/test'.format(job_name_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tensorflow model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step: A training step means using one batch size of training data to train the model.\n",
    "* Number of training steps per epoch: total_number_of_training_examples / batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "num_classes = 2\n",
    "mini_batch_size =  64\n",
    "max_steps = int(len(X_train) / mini_batch_size) * epoch\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "job_name = job_name_prefix + timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-2-755441266669/pneumonia-detection/input/tfrecord/train/\n",
      "s3://sagemaker-us-east-2-755441266669/pneumonia-detection/input/tfrecord/test/\n"
     ]
    }
   ],
   "source": [
    "input_prefix = '{0}/input/tfrecord'.format(job_name_prefix)\n",
    "input_train = 's3://{}/{}/train/'.format(bucket, input_prefix)\n",
    "input_test = 's3://{}/{}/test/'.format(bucket, input_prefix)\n",
    "output_prefix = '{0}/output'.format(job_name_prefix)\n",
    "output_path = 's3://{}/{}/'.format(bucket, output_prefix)\n",
    "\n",
    "print(input_train)\n",
    "print(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_count = 1\n",
    "instance_type = 'ml.p2.xlarge'\n",
    "volume_size_gb = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "train_timeout = 360000\n",
    "training_script_path = 'tensorflowScript.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a sagemaker.Tensorflow estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TensorFlow(entry_point=training_script_path,\n",
    "                       source_dir='source_tf_cnn_1',\n",
    "                       role=role,\n",
    "                       train_instance_count=instance_count,\n",
    "                       train_instance_type=instance_type,\n",
    "                       train_volume_size=volume_size_gb,\n",
    "                       train_max_run=train_timeout,\n",
    "                       model_dir=output_path,\n",
    "                       output_path=output_path,\n",
    "                       framework_version='1.12.0',\n",
    "                       py_version = 'py3',\n",
    "                       hyperparameters = {\n",
    "                           'num-classes': num_classes,\n",
    "                           'mini-batch-size': mini_batch_size,\n",
    "                           'max-steps': max_steps,\n",
    "                           'learning-rate': learning_rate\n",
    "                       },\n",
    "                       metric_definitions = [\n",
    "                           {\n",
    "                               'Name': 'loss',\n",
    "                               'Regex': 'loss = ([0-9\\\\.]+)'\n",
    "                           }\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-03 21:31:28 Starting - Starting the training job...\n",
      "2020-04-03 21:31:29 Starting - Launching requested ML instances...\n",
      "2020-04-03 21:32:25 Starting - Preparing the instances for training.........\n",
      "2020-04-03 21:33:41 Downloading - Downloading input data......\n",
      "2020-04-03 21:34:53 Training - Training image download completed. Training in progress..\u001b[34m2020-04-03 21:34:56,452 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2020-04-03 21:34:56,823 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"num-classes\": 2,\n",
      "        \"learning-rate\": 0.01,\n",
      "        \"max-steps\": 730,\n",
      "        \"model_dir\": \"s3://sagemaker-us-east-2-755441266669/pneumonia-detection/output/\",\n",
      "        \"mini-batch-size\": 64\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pneumonia-detection-2020-04-03-21-31-26\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-755441266669/pneumonia-detection-2020-04-03-21-31-26/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"tensorflowScript\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"tensorflowScript.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"learning-rate\":0.01,\"max-steps\":730,\"mini-batch-size\":64,\"model_dir\":\"s3://sagemaker-us-east-2-755441266669/pneumonia-detection/output/\",\"num-classes\":2}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=tensorflowScript.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=tensorflowScript\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-755441266669/pneumonia-detection-2020-04-03-21-31-26/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"learning-rate\":0.01,\"max-steps\":730,\"mini-batch-size\":64,\"model_dir\":\"s3://sagemaker-us-east-2-755441266669/pneumonia-detection/output/\",\"num-classes\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pneumonia-detection-2020-04-03-21-31-26\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-755441266669/pneumonia-detection-2020-04-03-21-31-26/source/sourcedir.tar.gz\",\"module_name\":\"tensorflowScript\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"tensorflowScript.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--learning-rate\",\"0.01\",\"--max-steps\",\"730\",\"--mini-batch-size\",\"64\",\"--model_dir\",\"s3://sagemaker-us-east-2-755441266669/pneumonia-detection/output/\",\"--num-classes\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_NUM-CLASSES=2\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=0.01\u001b[0m\n",
      "\u001b[34mSM_HP_MAX-STEPS=730\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://sagemaker-us-east-2-755441266669/pneumonia-detection/output/\u001b[0m\n",
      "\u001b[34mSM_HP_MINI-BATCH-SIZE=64\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python tensorflowScript.py --learning-rate 0.01 --max-steps 730 --mini-batch-size 64 --model_dir s3://sagemaker-us-east-2-755441266669/pneumonia-detection/output/ --num-classes 2\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Using default config.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpidi_hbew\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpidi_hbew', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\u001b[0m\n",
      "\u001b[34mgraph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6bd7eb4e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Not using Distribute Coordinator.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Running training and evaluation locally (non-distributed).\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From tensorflowScript.py:130: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:276: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:197: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From tensorflowScript.py:111: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From tensorflowScript.py:133: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Create CheckpointSaverHook.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Create CheckpointSaverHook.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpidi_hbew/model.ckpt.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpidi_hbew/model.ckpt.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:loss = 0.7698931, step = 0\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:loss = 0.7698931, step = 0\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:global_step/sec: 7.14356\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:global_step/sec: 7.14356\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:loss = 0.31677377, step = 100 (13.999 sec)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:loss = 0.31677377, step = 100 (13.999 sec)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:global_step/sec: 7.18472\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:global_step/sec: 7.18472\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:loss = 0.31232846, step = 200 (13.918 sec)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:loss = 0.31232846, step = 200 (13.918 sec)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:global_step/sec: 7.18163\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:global_step/sec: 7.18163\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:loss = 0.27525222, step = 300 (13.924 sec)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:loss = 0.27525222, step = 300 (13.924 sec)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:global_step/sec: 7.17442\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:global_step/sec: 7.17442\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:loss = 0.33688268, step = 400 (13.938 sec)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:loss = 0.33688268, step = 400 (13.938 sec)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:global_step/sec: 7.15052\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:global_step/sec: 7.15052\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:loss = 0.13153681, step = 500 (13.985 sec)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:loss = 0.13153681, step = 500 (13.985 sec)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:global_step/sec: 7.17674\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:global_step/sec: 7.17674\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:loss = 0.09868711, step = 600 (13.934 sec)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:loss = 0.09868711, step = 600 (13.934 sec)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:global_step/sec: 6.99664\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:global_step/sec: 6.99664\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:loss = 0.21337152, step = 700 (14.292 sec)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:loss = 0.21337152, step = 700 (14.292 sec)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Saving checkpoints for 730 into /tmp/tmpidi_hbew/model.ckpt.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Saving checkpoints for 730 into /tmp/tmpidi_hbew/model.ckpt.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Starting evaluation at 2020-04-03-21:36:47\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Starting evaluation at 2020-04-03-21:36:47\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Restoring parameters from /tmp/tmpidi_hbew/model.ckpt-730\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Restoring parameters from /tmp/tmpidi_hbew/model.ckpt-730\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [73/730]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [73/730]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [146/730]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [146/730]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [219/730]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [219/730]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [292/730]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [292/730]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [365/730]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [365/730]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [438/730]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [438/730]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [511/730]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [511/730]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [584/730]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [584/730]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [657/730]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [657/730]\u001b[0m\n",
      "\n",
      "2020-04-03 21:37:51 Uploading - Uploading generated training model\u001b[34mINFO:tensorflow:Evaluation [730/730]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [730/730]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Finished evaluation at 2020-04-03-21:37:47\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Finished evaluation at 2020-04-03-21:37:47\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Saving dict for global step 730: accuracy = 0.9434931, global_step = 730, loss = 0.15335508\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Saving dict for global step 730: accuracy = 0.9434931, global_step = 730, loss = 0.15335508\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Saving 'checkpoint_path' summary for global step 730: /tmp/tmpidi_hbew/model.ckpt-730\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Saving 'checkpoint_path' summary for global step 730: /tmp/tmpidi_hbew/model.ckpt-730\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Loss for final step: 0.08537624.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Loss for final step: 0.08537624.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Classify: None\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Classify: None\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Regress: None\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Regress: None\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Predict: ['predictions', 'serving_default']\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Predict: ['predictions', 'serving_default']\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Train: None\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Train: None\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Eval: None\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Eval: None\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Restoring parameters from /tmp/tmpidi_hbew/model.ckpt-730\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Restoring parameters from /tmp/tmpidi_hbew/model.ckpt-730\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py:1046: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPass your op to the equivalent parameter main_op instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py:1046: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPass your op to the equivalent parameter main_op instead.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets added to graph.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets added to graph.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:No assets to write.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:No assets to write.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:SavedModel written to: /opt/ml/model/temp-b'1585949867'/saved_model.pb\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:SavedModel written to: /opt/ml/model/temp-b'1585949867'/saved_model.pb\u001b[0m\n",
      "\u001b[34m2020-04-03 21:37:49,561 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-04-03 21:38:08 Completed - Training job completed\n",
      "Training seconds: 267\n",
      "Billable seconds: 267\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({\n",
    "    'train': input_train,\n",
    "    'test': input_test\n",
    "}, job_name = job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "instance_count = 1\n",
    "instance_type = 'ml.m4.xlarge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_prefix = 'pnu-image-classification-tensorflow-cnn-1'\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "model_name = model_name_prefix + timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artifacts_s3_path = 's3://{0}/{1}/output/{2}/output/model.tar.gz'\\\n",
    ".format(bucket, \n",
    "        'pneumonia-detection',\n",
    "        'pneumonia-detection-2020-04-03-17-35-56'\n",
    "       )\n",
    "model = Model(\n",
    "    name=model_name,\n",
    "    model_data=model_artifacts_s3_path,\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "endpoint_name_prefix = 'pneumonia-detection-ep'\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_name = endpoint_name_prefix + timestamp\n",
    "\n",
    "predictor = model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=instance_count,\n",
    "    instance_type=instance_type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing deployed model on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ec2-user/SageMaker/chest_xray_standard/val/NORMAL/NORMAL2-IM-1437-0001.jpeg',\n",
       " '/home/ec2-user/SageMaker/chest_xray_standard/val/PNEUMONIA/person1951_bacteria_4882.jpeg',\n",
       " '/home/ec2-user/SageMaker/chest_xray_standard/val/NORMAL/NORMAL2-IM-1431-0001.jpeg',\n",
       " '/home/ec2-user/SageMaker/chest_xray_standard/val/NORMAL/NORMAL2-IM-1440-0001.jpeg',\n",
       " '/home/ec2-user/SageMaker/chest_xray_standard/val/PNEUMONIA/person1946_bacteria_4874.jpeg',\n",
       " '/home/ec2-user/SageMaker/chest_xray_standard/val/PNEUMONIA/person1949_bacteria_4880.jpeg',\n",
       " '/home/ec2-user/SageMaker/chest_xray_standard/val/PNEUMONIA/person1947_bacteria_4876.jpeg',\n",
       " '/home/ec2-user/SageMaker/chest_xray_standard/val/NORMAL/NORMAL2-IM-1442-0001.jpeg',\n",
       " '/home/ec2-user/SageMaker/chest_xray_standard/val/NORMAL/NORMAL2-IM-1438-0001.jpeg',\n",
       " '/home/ec2-user/SageMaker/chest_xray_standard/val/NORMAL/NORMAL2-IM-1427-0001.jpeg']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_root = pathlib.Path(os.path.join(os.getcwd(), 'chest_xray_standard'))\n",
    "all_image_paths = list(data_root.glob('val/*/*'))\n",
    "all_image_paths = [str(path) for path in all_image_paths]\n",
    "random.shuffle(all_image_paths)\n",
    "image_count = len(all_image_paths)\n",
    "\n",
    "all_image_paths[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NORMAL', 'PNEUMONIA', 'NORMAL', 'NORMAL', 'PNEUMONIA', 'PNEUMONIA', 'PNEUMONIA', 'NORMAL', 'NORMAL', 'NORMAL']\n",
      "[0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "all_image_labels = [pathlib.Path(path).parent.name\n",
    "                    for path in all_image_paths]\n",
    "print(all_image_labels[:10])\n",
    "all_image_labels = list(map((lambda v: 1 if v == 'PNEUMONIA' else 0), \n",
    "                            all_image_labels))\n",
    "print(all_image_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 3\n",
    "image_height = 224\n",
    "image_width = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  (224, 224, 3)\n",
      "x:  (224, 224, 3)\n",
      "x:  (224, 224, 3)\n",
      "x:  (224, 224, 3)\n",
      "x:  (224, 224, 3)\n",
      "x:  (224, 224, 3)\n",
      "x:  (224, 224, 3)\n",
      "x:  (224, 224, 3)\n",
      "x:  (224, 224, 3)\n",
      "x:  (224, 224, 3)\n",
      "x:  (224, 224, 3)\n",
      "x:  (224, 224, 3)\n",
      "x:  (224, 224, 3)\n",
      "x:  (224, 224, 3)\n",
      "x:  (224, 224, 3)\n",
      "x:  (224, 224, 3)\n",
      "All images to array!\n"
     ]
    }
   ],
   "source": [
    "dataset = np.ndarray(shape=(image_count, image_height, image_width, channels),\n",
    "                     dtype=np.uint8)\n",
    "\n",
    "i = 0\n",
    "for file in all_image_paths:\n",
    "    img = load_img(file)  # this is a PIL image\n",
    "    \n",
    "    img = img.resize((image_width, image_height))\n",
    "    x = img_to_array(img, 'channels_last')\n",
    "    print(\"x: \", x.shape)\n",
    "    dataset[i] = x\n",
    "    i += 1\n",
    "    if i % 250 == 0:\n",
    "        print(\"%d images to array\" % i)\n",
    "print(\"All images to array!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pneumonia(image):\n",
    "    response = predictor.predict(image)\n",
    "    print('Received response is: ', response)\n",
    "    print('Probabilities for all classes: ',\n",
    "          response['predictions'][0]['probabilities'])\n",
    "    predicted_class = response['predictions'][0]['classes']\n",
    "    if predicted_class == 0:\n",
    "        print('Pneumonia not detected')\n",
    "        return 0\n",
    "    else:\n",
    "        print('Pneumonia detected')\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_with_no_pnu = dataset[np.logical_not(all_image_labels)][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_with_pnu = dataset[all_image_labels][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response is:  {'predictions': [{'probabilities': [1.0, 0.0], 'classes': 0}]}\n",
      "Probabilities for all classes:  [1.0, 0.0]\n",
      "Pneumonia not detected\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_pneumonia(image_with_no_pnu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response is:  {'predictions': [{'probabilities': [2.49911e-38, 1.0], 'classes': 1}]}\n",
      "Probabilities for all classes:  [2.49911e-38, 1.0]\n",
      "Pneumonia detected\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_pneumonia(image_with_pnu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get prediction labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metrics(confusion_matrix):\n",
    "    tn, fn, fp, tp = confusion_matrix[0][0], confusion_matrix[0][1], confusion_matrix[1][0], confusion_matrix[1][1]\n",
    "    accuracy = sum([tn, tp])/sum([tn, fn, fp, tp])\n",
    "    precision = sum([tp])/ sum([tp, fp])\n",
    "    recall = sum([tp])/ sum([tp, fn])\n",
    "    f1_score = 2 * ((precision * recall)/(precision + recall))\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "    return accuracy, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response is:  {'predictions': [{'probabilities': [2.49911e-38, 1.0], 'classes': 1}]}\n",
      "Probabilities for all classes:  [2.49911e-38, 1.0]\n",
      "Pneumonia detected\n",
      "Received response is:  {'predictions': [{'classes': 1, 'probabilities': [0.0, 1.0]}]}\n",
      "Probabilities for all classes:  [0.0, 1.0]\n",
      "Pneumonia detected\n",
      "Received response is:  {'predictions': [{'probabilities': [1.0, 0.0], 'classes': 0}]}\n",
      "Probabilities for all classes:  [1.0, 0.0]\n",
      "Pneumonia not detected\n",
      "Received response is:  {'predictions': [{'probabilities': [1.0, 0.0], 'classes': 0}]}\n",
      "Probabilities for all classes:  [1.0, 0.0]\n",
      "Pneumonia not detected\n",
      "Received response is:  {'predictions': [{'probabilities': [0.0, 1.0], 'classes': 1}]}\n",
      "Probabilities for all classes:  [0.0, 1.0]\n",
      "Pneumonia detected\n",
      "Received response is:  {'predictions': [{'classes': 0, 'probabilities': [1.0, 0.0]}]}\n",
      "Probabilities for all classes:  [1.0, 0.0]\n",
      "Pneumonia not detected\n",
      "Received response is:  {'predictions': [{'probabilities': [0.0, 1.0], 'classes': 1}]}\n",
      "Probabilities for all classes:  [0.0, 1.0]\n",
      "Pneumonia detected\n",
      "Received response is:  {'predictions': [{'probabilities': [1.0, 0.0], 'classes': 0}]}\n",
      "Probabilities for all classes:  [1.0, 0.0]\n",
      "Pneumonia not detected\n",
      "Received response is:  {'predictions': [{'probabilities': [0.0, 1.0], 'classes': 1}]}\n",
      "Probabilities for all classes:  [0.0, 1.0]\n",
      "Pneumonia detected\n",
      "Received response is:  {'predictions': [{'probabilities': [1.0, 0.0], 'classes': 0}]}\n",
      "Probabilities for all classes:  [1.0, 0.0]\n",
      "Pneumonia not detected\n",
      "Received response is:  {'predictions': [{'probabilities': [1.63636e-09, 1.0], 'classes': 1}]}\n",
      "Probabilities for all classes:  [1.63636e-09, 1.0]\n",
      "Pneumonia detected\n",
      "Received response is:  {'predictions': [{'probabilities': [1.0, 0.0], 'classes': 0}]}\n",
      "Probabilities for all classes:  [1.0, 0.0]\n",
      "Pneumonia not detected\n",
      "Received response is:  {'predictions': [{'probabilities': [0.0, 1.0], 'classes': 1}]}\n",
      "Probabilities for all classes:  [0.0, 1.0]\n",
      "Pneumonia detected\n",
      "Received response is:  {'predictions': [{'classes': 1, 'probabilities': [0.0, 1.0]}]}\n",
      "Probabilities for all classes:  [0.0, 1.0]\n",
      "Pneumonia detected\n",
      "Received response is:  {'predictions': [{'probabilities': [1.0, 0.0], 'classes': 0}]}\n",
      "Probabilities for all classes:  [1.0, 0.0]\n",
      "Pneumonia not detected\n",
      "Received response is:  {'predictions': [{'probabilities': [0.0, 1.0], 'classes': 1}]}\n",
      "Probabilities for all classes:  [0.0, 1.0]\n",
      "Pneumonia detected\n"
     ]
    }
   ],
   "source": [
    "predictions = list([ predict_pneumonia(data) for data in dataset ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>prediction</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "prediction  0  1\n",
       "actual          \n",
       "0           6  2\n",
       "1           1  7"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix = pd.crosstab(index=np.array(all_image_labels), columns=np.array(predictions),\n",
    "                               rownames=['actual'], colnames=['prediction'])\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8125\n",
      "Precision: 0.7777777777777778\n",
      "Recall: 0.875\n",
      "F1 Score: 0.823529411764706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8125, 0.7777777777777778, 0.875, 0.823529411764706)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_metrics(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
